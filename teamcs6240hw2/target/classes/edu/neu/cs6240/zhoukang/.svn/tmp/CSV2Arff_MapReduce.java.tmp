package edu.neu.cs6240.zhoukang;

import java.io.*;
import java.util.*;
import java.nio.charset.Charset;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
//import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
//import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
//import org.apache.hadoop.util.*;
//import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.util.GenericOptionsParser;

import weka.core.Instances;
import weka.core.converters.ArffSaver;
import weka.core.converters.CSVLoader;

public class CSV2Arff_MapReduce {

	public static class Map extends
			Mapper<LongWritable, Text, IntWritable, Text> {
		// private java.util.List<CharSequence> lines;
		private StringBuilder sb;

		protected void setup(Context context) {
			sb = new StringBuilder();
		}

		public void map(LongWritable key, Text value, Context context)
				throws IOException, InterruptedException {
			sb.append(value.toString() + "\n");
		}

		protected void cleanup(Context context) throws IOException,
				InterruptedException {
			// context.write(new IntWritable(1), new Text(sb.toString()));

			String ascii = "us-ascii";

			CSVLoader loader = new CSVLoader();
			loader.setSource(new ByteArrayInputStream(sb.toString().getBytes(
					Charset.forName("us-ascii"))));
			Instances data = loader.getDataSet();

			ByteArrayOutputStream baos = new ByteArrayOutputStream();
			ArffSaver saver = new ArffSaver();
			saver.setInstances(data);
			// saver.setFile(new File(args[1]));
			saver.setDestination(baos);
			saver.writeBatch();

			context.write(new IntWritable(1), new Text(baos.toString(ascii)));
		}
	}

	// TODO: merge CSV Header
	public static class Reduce extends
			Reducer<IntWritable, Text, IntWritable, Text> {
		public void reduce(IntWritable key, Iterable<Text> values,
				Context context) throws IOException, InterruptedException {
			for (Text t : values) {
				context.write(key, t);
			}
		}
	}

	public static PrintWriter createNextWriter(File ofolder, File file,
			int count, String firstLine) throws FileNotFoundException {
		PrintWriter writer = new PrintWriter(new File(ofolder, file.getName()
				+ ".part_" + count++));
		writer.println(firstLine);
		return writer;
	}

	public static void splitInputFiles(File file, File ofolder)
			throws IOException {
		int lineCount = 5000;
		if (file.isDirectory()) {
			for (File f : file.listFiles()) {
				splitInputFiles(f, ofolder);
			}
			return;
		}

<<<<<<< .mine
		BufferedReader r = new BufferedReader(new FileReader(file));
		String firstLine = r.readLine();

		String line;
		int partCount = 1;
		PrintWriter writer = null;
		for (int lineno = 1; (line = r.readLine()) != null; lineno++) {
			if (writer == null) {
				writer = createNextWriter(ofolder, file, partCount++, firstLine);
			}
			writer.println(line);
			if (lineno % lineCount == 0) {
				writer.close();
			}
		}
		writer.close();
=======
	String line;
	int partCount = 1;
	PrintWriter writer = null;
	for(int lineno=1; (line = r.readLine())!=null; lineno++){
	    if(writer==null){ writer = createNextWriter(ofolder, file, partCount++, firstLine); }
	    writer.println(line);
	    if(lineno % lineCount ==0){ writer.close(); writer=null; }
>>>>>>> .r20
	}
<<<<<<< .mine
=======
	if(writer !=null){ writer.close(); }
    }
>>>>>>> .r20

	/**
	 * takes 2 arguments: - CSV input file - ARFF output file
	 */
	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		// String[] otherArgs = (new GenericOptionsParser(conf,
		// args)).getRemainingArgs();
		if (args.length != 3) {
			System.err.println("Usage: wordcount <in> <out> <tmp>");
			System.exit(2);
		}

		File tmpFolder = new File(args[2]);
		if (!tmpFolder.isDirectory()) {
			throw new RuntimeException("args[2] must be a tmp DIRECTORY");
		}
		splitInputFiles(new File(args[0]), tmpFolder);
		System.err.println("Finished splitting Input");

		Job job = new Job(conf, "word count");
		job.setJarByClass(CSV2Arff_MapReduce.class);
		job.setMapperClass(Map.class);
		// job.setCombinerClass(Combine.class);
		job.setReducerClass(Reduce.class);
		job.setOutputKeyClass(IntWritable.class);
		job.setOutputValueClass(Text.class);
		job.setInputFormatClass(UnsplittableTextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		FileInputFormat.addInputPath(job, new Path(args[2]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}
}
